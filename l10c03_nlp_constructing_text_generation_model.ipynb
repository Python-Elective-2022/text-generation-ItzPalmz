{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "a262e445-7820-4532-c327-b933f6012869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-19 08:27:10--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.20.138, 74.125.20.100, 74.125.20.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.20.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/u1dapb2cm9aj8n4ut2u89ncrfi6k3f7a/1679214375000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=8a180c5c-8619-4ec5-9965-93780d562dd7 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-19 08:27:14--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/u1dapb2cm9aj8n4ut2u89ncrfi6k3f7a/1679214375000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=8a180c5c-8619-4ec5-9965-93780d562dd7\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 173.194.203.132, 2607:f8b0:400e:c05::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|173.194.203.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M  5.23MB/s    in 13s     \n",
            "\n",
            "2023-03-19 08:27:30 (5.23 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "91a4f5f7-26ae-4eb0-e0c1-950e96672faa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "cf17b708-72af-4f8b-89c8-7d64920c0d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "be4eb5d2-b394-4371-fa76-1ea54020ffe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 19s 96ms/step - loss: 5.8043 - accuracy: 0.0293\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 20ms/step - loss: 5.3947 - accuracy: 0.0399\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.2736 - accuracy: 0.0459\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.1097 - accuracy: 0.0560\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8976 - accuracy: 0.0691\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.6094 - accuracy: 0.0959\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3337 - accuracy: 0.1034\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.0471 - accuracy: 0.1448\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7580 - accuracy: 0.1877\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4712 - accuracy: 0.2119\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.1538 - accuracy: 0.2664\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8643 - accuracy: 0.3295\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6283 - accuracy: 0.3562\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.3685 - accuracy: 0.4132\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.1570 - accuracy: 0.4617\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 1.9552 - accuracy: 0.5020\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 1.7810 - accuracy: 0.5580\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6329 - accuracy: 0.5913\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4987 - accuracy: 0.6256\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.3971 - accuracy: 0.6504\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2637 - accuracy: 0.6826\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.1515 - accuracy: 0.7053\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0699 - accuracy: 0.7260\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 0.9835 - accuracy: 0.7412\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9046 - accuracy: 0.7785\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8364 - accuracy: 0.7815\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7856 - accuracy: 0.7967\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7301 - accuracy: 0.8088\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6858 - accuracy: 0.8234\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6221 - accuracy: 0.8305\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5851 - accuracy: 0.8466\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5551 - accuracy: 0.8512\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5309 - accuracy: 0.8557\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5176 - accuracy: 0.8537\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4615 - accuracy: 0.8764\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4402 - accuracy: 0.8779\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4203 - accuracy: 0.8819\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4016 - accuracy: 0.8875\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3971 - accuracy: 0.8804\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.3768 - accuracy: 0.8860\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3702 - accuracy: 0.8855\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3548 - accuracy: 0.8900\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3494 - accuracy: 0.8966\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3295 - accuracy: 0.9001\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3137 - accuracy: 0.9021\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3120 - accuracy: 0.8981\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3063 - accuracy: 0.8966\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3078 - accuracy: 0.8986\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2977 - accuracy: 0.8971\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2910 - accuracy: 0.8966\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2890 - accuracy: 0.8966\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.2930 - accuracy: 0.9001\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.2804 - accuracy: 0.8996\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.2868 - accuracy: 0.9021\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.2902 - accuracy: 0.8956\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.2800 - accuracy: 0.8961\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2796 - accuracy: 0.9016\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2732 - accuracy: 0.9057\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2734 - accuracy: 0.8961\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2806 - accuracy: 0.8981\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3054 - accuracy: 0.8925\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2882 - accuracy: 0.8986\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.2736 - accuracy: 0.9026\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2642 - accuracy: 0.8971\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2561 - accuracy: 0.9036\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2548 - accuracy: 0.8981\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2566 - accuracy: 0.9041\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2586 - accuracy: 0.9026\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2517 - accuracy: 0.9016\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2546 - accuracy: 0.9006\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2636 - accuracy: 0.8956\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2508 - accuracy: 0.9006\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2556 - accuracy: 0.9026\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2585 - accuracy: 0.8971\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2409 - accuracy: 0.9041\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2459 - accuracy: 0.8976\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2454 - accuracy: 0.8976\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2440 - accuracy: 0.9001\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2404 - accuracy: 0.9051\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2417 - accuracy: 0.8991\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2589 - accuracy: 0.8966\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2503 - accuracy: 0.9046\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.2586 - accuracy: 0.8935\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2935 - accuracy: 0.8840\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2867 - accuracy: 0.8915\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2565 - accuracy: 0.8966\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2570 - accuracy: 0.8956\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2454 - accuracy: 0.8986\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2412 - accuracy: 0.8996\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2368 - accuracy: 0.8971\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2388 - accuracy: 0.9011\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2338 - accuracy: 0.9046\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2313 - accuracy: 0.9021\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2329 - accuracy: 0.8996\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2383 - accuracy: 0.9026\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2325 - accuracy: 0.8981\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2326 - accuracy: 0.9016\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2319 - accuracy: 0.9001\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2281 - accuracy: 0.9046\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2310 - accuracy: 0.9062\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2253 - accuracy: 0.9036\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2318 - accuracy: 0.9031\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2303 - accuracy: 0.9001\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2266 - accuracy: 0.9021\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2281 - accuracy: 0.9001\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2266 - accuracy: 0.9026\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2246 - accuracy: 0.9046\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.2310 - accuracy: 0.9016\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2304 - accuracy: 0.8991\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2250 - accuracy: 0.9041\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2273 - accuracy: 0.9026\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2286 - accuracy: 0.9031\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2214 - accuracy: 0.9026\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2217 - accuracy: 0.9067\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2286 - accuracy: 0.9031\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2270 - accuracy: 0.9006\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2227 - accuracy: 0.9016\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2223 - accuracy: 0.9021\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2239 - accuracy: 0.9067\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2187 - accuracy: 0.9046\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2201 - accuracy: 0.9041\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2261 - accuracy: 0.9016\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2266 - accuracy: 0.8991\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2233 - accuracy: 0.8961\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2207 - accuracy: 0.9036\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2476 - accuracy: 0.8976\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2843 - accuracy: 0.8940\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2972 - accuracy: 0.8824\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3249 - accuracy: 0.8718\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2920 - accuracy: 0.8824\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2353 - accuracy: 0.8971\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2218 - accuracy: 0.9046\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2194 - accuracy: 0.9051\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2187 - accuracy: 0.9021\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2195 - accuracy: 0.9036\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2169 - accuracy: 0.9016\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2151 - accuracy: 0.9016\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2150 - accuracy: 0.9057\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2145 - accuracy: 0.9041\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2167 - accuracy: 0.9026\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2145 - accuracy: 0.9072\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2153 - accuracy: 0.9011\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2158 - accuracy: 0.9051\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2118 - accuracy: 0.9097\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2133 - accuracy: 0.9046\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2111 - accuracy: 0.9062\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2135 - accuracy: 0.9026\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2111 - accuracy: 0.9057\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2136 - accuracy: 0.9062\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2164 - accuracy: 0.9036\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2129 - accuracy: 0.9051\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2125 - accuracy: 0.9046\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2129 - accuracy: 0.9046\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.2104 - accuracy: 0.9036\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2104 - accuracy: 0.9041\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2087 - accuracy: 0.9036\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2109 - accuracy: 0.9057\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2106 - accuracy: 0.9031\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2130 - accuracy: 0.9031\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2104 - accuracy: 0.9046\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2074 - accuracy: 0.9046\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2084 - accuracy: 0.9036\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2089 - accuracy: 0.9072\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2091 - accuracy: 0.9077\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2104 - accuracy: 0.9077\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2110 - accuracy: 0.9031\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2188 - accuracy: 0.9001\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2131 - accuracy: 0.9057\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2103 - accuracy: 0.9067\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2083 - accuracy: 0.9057\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2183 - accuracy: 0.9011\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2769 - accuracy: 0.8880\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2707 - accuracy: 0.8925\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2607 - accuracy: 0.8946\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2294 - accuracy: 0.9011\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2103 - accuracy: 0.9062\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2107 - accuracy: 0.9062\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2073 - accuracy: 0.9051\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2087 - accuracy: 0.9067\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2060 - accuracy: 0.9036\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2090 - accuracy: 0.9062\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2074 - accuracy: 0.9051\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2048 - accuracy: 0.9057\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2040 - accuracy: 0.9051\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.2054 - accuracy: 0.9041\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2040 - accuracy: 0.9082\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2040 - accuracy: 0.9057\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2028 - accuracy: 0.9057\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2049 - accuracy: 0.9067\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2043 - accuracy: 0.9046\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2044 - accuracy: 0.9046\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2022 - accuracy: 0.9062\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2024 - accuracy: 0.9062\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2041 - accuracy: 0.9072\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2031 - accuracy: 0.9092\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2052 - accuracy: 0.9051\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2054 - accuracy: 0.9031\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2026 - accuracy: 0.9067\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2025 - accuracy: 0.9072\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2012 - accuracy: 0.9092\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='relu'))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "15f52603-5ded-48e7-e85b-a41ef00f6264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "7cbd5f3a-dbcb-45b4-b907-cffd73939561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 883ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "we wish you a future you but still moving on give yourself a break life of give and take me would weave pain will advice is to take good care my break intention life is girl life to without she blue smiles makes good care care my intention feel fine fine fine life is yourself a break life is girl life to without she blue she ways ways ways ways be so ways ways quiet here i am again am again i am again am again i am again am again your am again am a love i love i had the rotten the\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"we wish you a\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}